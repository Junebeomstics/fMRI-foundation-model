{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d078f7e7-ccbc-4a88-9d12-93510fe1814b",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56bbd92-27d1-4f94-a579-029661eb72f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import sys\n",
    "from subprocess import call\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import boto3\n",
    "import webdataset as wds\n",
    "import nibabel as nib\n",
    "import pickle as pkl\n",
    "from einops import rearrange\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchio as tio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c1ba3-39ff-458c-bc3c-e452445eb10c",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46adfbe0-91ef-43ae-87a9-c2e4ea89033a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape_to_2d(tensor):\n",
    "    return rearrange(tensor, 'b h w c -> (b h) (c w)')\n",
    "\n",
    "def reshape_to_original(tensor_2d, b=300, h=64, w=64, c=48):\n",
    "    return rearrange(tensor_2d, '(b h) (c w) -> b h w c', b=b, h=h, w=w, c=c)\n",
    "\n",
    "def header_to_dict(header):\n",
    "    readable_header = {}\n",
    "    for key, value in header.items():\n",
    "        readable_header[key] = value\n",
    "    return readable_header\n",
    "\n",
    "def temporal_interp1d(fmri_data, change_TR):\n",
    "    original_time_points = np.arange(fmri_data.shape[0])  # Time points: 0, 1, 2, ..., T-1\n",
    "    new_time_points = np.arange(0, fmri_data.shape[0], change_TR)  # New time points: 0, 2, 4, ...\n",
    "\n",
    "    reshaped_data = fmri_data.reshape(fmri_data.shape[0], -1)  # Reshape to (T, X*Y*Z)\n",
    "    interpolate = interp1d(original_time_points, reshaped_data, kind='linear', axis=0, bounds_error=False, fill_value=\"extrapolate\")\n",
    "    resampled_fmri_data = interpolate(new_time_points).reshape((len(new_time_points),) + fmri_data.shape[1:])\n",
    "    return resampled_fmri_data\n",
    "\n",
    "def list_folders(bucket, prefix='', delimiter='/'):\n",
    "    folder_names = []\n",
    "    continuation_token = None\n",
    "    while True:\n",
    "        # Include the continuation token in the request if it exists\n",
    "        kwargs = {'Bucket': bucket, 'Prefix': prefix, 'Delimiter': delimiter}\n",
    "        if continuation_token:\n",
    "            kwargs['ContinuationToken'] = continuation_token\n",
    "\n",
    "        response = s3.list_objects_v2(**kwargs)\n",
    "        folder_names.extend([x['Prefix'].split('/')[-2] for x in response.get('CommonPrefixes', [])])\n",
    "\n",
    "        # Check if more items are available to retrieve\n",
    "        if 'NextContinuationToken' in response:\n",
    "            continuation_token = response['NextContinuationToken']\n",
    "        else:\n",
    "            break\n",
    "    return folder_names\n",
    "\n",
    "def list_all_objects(bucket, prefix):\n",
    "    continuation_token = None\n",
    "    while True:\n",
    "        if continuation_token:\n",
    "            response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, ContinuationToken=continuation_token)\n",
    "        else:\n",
    "            response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "        for content in response.get('Contents', []):\n",
    "            yield content\n",
    "\n",
    "        continuation_token = response.get('NextContinuationToken')\n",
    "        if not continuation_token:\n",
    "            break\n",
    "\n",
    "def torchio_slice(data,xslice=None,yslice=None,zslice=None):    \n",
    "    if xslice is None: xslice = data.shape[1] // 2\n",
    "    if yslice is None: yslice = data.shape[2] // 2\n",
    "    if zslice is None: zslice = data.shape[3] // 2\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(5,5))\n",
    "\n",
    "    # Plot the three different slices\n",
    "    axs[0].imshow(data[0, xslice], cmap='gray')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(f'Slice [0, {xslice}]', fontsize=8)\n",
    "\n",
    "    axs[1].imshow(data[0, :, yslice], cmap='gray')\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title(f'Slice [0, :, {yslice}]', fontsize=8)\n",
    "\n",
    "    axs[2].imshow(data[0, :, :, zslice], cmap='gray')\n",
    "    axs[2].axis('off')\n",
    "    axs[2].set_title(f'Slice [0, :, :, {zslice}]', fontsize=8)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8f6f9-ce59-46fc-9b77-43d6ce5a3920",
   "metadata": {},
   "source": [
    "## Create dir to save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852d060-955e-47ec-ad04-1a285ee38059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "proj_name = \"openneuro\"\n",
    "outpath=f\"/fsx/proj-fmri/conscioustahoe/fMRI-foundation-model/dataset_creation/{proj_name}\"\n",
    "os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
    "os.makedirs(f\"{outpath}/tars\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a054802-8edd-4e37-9824-94d354eee709",
   "metadata": {},
   "source": [
    "## Sync metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358944ba-a39b-43b9-a4ce-b03b34c1e4fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "```bash\n",
    "aws s3 rm  s3://proj-fmri/fmri_foundation_datasets/conscioustahoe/openneuro/ --recursive\n",
    "\n",
    "aws s3 cp s3://proj-fmri/fmri_foundation_datasets/openneuro/metadata.json s3://proj-fmri/fmri_foundation_datasets/conscioustahoe/openneuro/ --region us-west-2\n",
    "\n",
    "aws s3 ls  s3://proj-fmri/fmri_foundation_datasets/conscioustahoe/openneuro/\n",
    "\n",
    "aws s3 sync s3://proj-fmri/fmri_foundation_datasets/openneuro/tars s3://proj-fmri/fmri_foundation_datasets/conscioustahoe/openneuro --region us-west-2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c684fb6-30f5-4413-97bc-91eac75e1aa7",
   "metadata": {},
   "source": [
    "## Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd67c0-011b-4d49-9fc6-4c14a21d41f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataset_creation_job():\n",
    "    # Below code assumes you are resuming your data creation from killed job\n",
    "\n",
    "    # copy metadata.json from s3 to /scratch\n",
    "    command = f\"aws s3 cp s3://proj-fmri/fmri_foundation_datasets/conscioustahoe/{proj_name}/metadata.json {outpath}/tars/metadata.json --region us-west-2\"\n",
    "    call(command,shell=True)\n",
    "\n",
    "    # read the metadata file\n",
    "    with open(f\"{outpath}/tars/metadata.json\", 'r') as file:\n",
    "        metadata = json.load(file)\n",
    "\n",
    "    TR_count = metadata['TR_count']\n",
    "    subj_count = metadata['subj_count']\n",
    "    tar_count = metadata['tar_count'] + 1\n",
    "    dataset_count = metadata['dataset_count']\n",
    "    obj_key_list = metadata['obj_key_list']\n",
    "    dataset_list = metadata['datasets']\n",
    "    failed_dataset_dict = metadata ['failed_datasets'] if 'failed_datasets' in metadata else {}\n",
    "\n",
    "    print(f\"TR_count: {TR_count}\")\n",
    "    print(f\"subj_count: {subj_count}\")\n",
    "    print(f\"tar_count: {tar_count}\")\n",
    "    print(f\"dataset_count: {dataset_count}\")\n",
    "    # print(f\"obj_key_list: {obj_key_list}\")\n",
    "    # print(f\"dataset_list: {dataset_list}\")        \n",
    "\n",
    "    # delete existing tars\n",
    "    print(\"deleting existing tars\")\n",
    "    command = f\"rm {outpath}/tars/*.tar\"\n",
    "    call(command,shell=True)\n",
    "\n",
    "    TRs_per_sample = 24\n",
    "    max_samples_per_tar = 320 # translates to around 1 Gb per tar\n",
    "    max_TRs_per_tar = max_samples_per_tar * TRs_per_sample\n",
    "\n",
    "    # Connect to S3\n",
    "    # s3 = boto3.client('s3')\n",
    "\n",
    "    # Set the bucket name and folder name\n",
    "    bucket_name = 'openneuro.org'\n",
    "    folder_names = list_folders(bucket_name)\n",
    "    NUM_DATASETS = len(folder_names)\n",
    "\n",
    "    print(f\"NUM_DATASETS: {NUM_DATASETS} TRs_per_sample: {TRs_per_sample} \\\n",
    "    max_samples_per_tar: {max_samples_per_tar} max_TRs_per_tar: {max_TRs_per_tar}\")\n",
    "\n",
    "    # # If you want to start over webdataset creation from the beginning, uncomment these:\n",
    "    # tar_count = 0\n",
    "    # TR_count = 0\n",
    "    # subj_count = 0\n",
    "    # dataset_count = 0\n",
    "    # dataset_list = []\n",
    "    # obj_key_list = []\n",
    "\n",
    "    sample_idx = 0\n",
    "    current_dataset = None\n",
    "    current_subject = None\n",
    "    sink = wds.TarWriter(f\"{outpath}/tars/{tar_count:06d}.tar\")\n",
    "\n",
    "    tio_transforms = tio.Compose(\n",
    "                    (\n",
    "                        tio.ToCanonical(), # make sure orientation of brains are consistent (RAS+ orientation)\n",
    "                        tio.RescaleIntensity(out_min_max=(0, 1)),\n",
    "                        tio.Resample(3, image_interpolation='nearest'), # rescale voxels to #mm isotropic\n",
    "                        tio.CropOrPad((64, 64, 48)),\n",
    "                    )\n",
    "                )\n",
    "    # tio_image = tio.ScalarImage(filename, dtype=np.float32)\n",
    "\n",
    "    try:\n",
    "        for folder_name in folder_names:\n",
    "\n",
    "            if folder_name in dataset_list or folder_name in failed_dataset_dict:\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing dataset: {folder_name}.\")\n",
    "            # List all objects in the folder\n",
    "            all_objects = list_all_objects(bucket_name, folder_name)\n",
    "            for obj in all_objects:\n",
    "                obj_key = obj['Key']\n",
    "\n",
    "        #         if '_T1w.nii.gz' in obj_key: # Anatomical\n",
    "        #             # Store subject number to verify anat/func match\n",
    "        #             anat_subj = obj_key.split('/')[1]\n",
    "\n",
    "        #             # Download the object to tmp location\n",
    "        #             filename = os.path.join('openneuro', obj_key)\n",
    "        #             os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "        #             s3.download_file(bucket_name, obj_key, filename)\n",
    "\n",
    "        #             # store the header of anat\n",
    "        #             anat_header = nib.load(filename).header\n",
    "\n",
    "                if '_bold.nii.gz' in obj_key:\n",
    "                    func_subj = obj_key.split('/')[1]\n",
    "\n",
    "                    # Verify func/anat subject number match\n",
    "                    # if anat_subj != func_subj:\n",
    "                    #     print('Incompatible subject number found. Skipping...')\n",
    "                    #     continue\n",
    "\n",
    "                    # if metadata file shows you already processed this file, skip it\n",
    "                    if np.isin(obj_key, obj_key_list):\n",
    "                        continue\n",
    "\n",
    "                    filename = os.path.join(f'{os.getcwd()}/openneuro', obj_key)\n",
    "\n",
    "                    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                    if not os.path.exists(filename):\n",
    "                        s3.download_file(bucket_name, obj_key, filename)\n",
    "\n",
    "                    func_nii = nib.load(filename)\n",
    "                    try:\n",
    "                        print(obj_key, func_nii.get_fdata().shape, \"| TRs:\", TR_count, \"samp:\", sample_idx)\n",
    "                    except Exception as e:\n",
    "                        print(f\"get_fdata() error occurred: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        tio_image = tio.ScalarImage(tensor=np.moveaxis(func_nii.get_fdata(),-1,0).astype(np.float32), \n",
    "                                                affine=func_nii.affine, \n",
    "                                                dtype=np.float32)\n",
    "                        out = tio_transforms(tio_image)['data']\n",
    "                    except Exception as e: # this can happen if the func is actually 3d and not 4d\n",
    "                        print(f\"tio processing error occurred: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    # # discard datasets with unusual Repetition Time\n",
    "                    # orig_TR = func_nii.header.get_zooms()[3]\n",
    "                    # if orig_TR > 100: # assuming it is in milliseconds\n",
    "                    #     orig_TR = orig_TR / 1000\n",
    "                    # if orig_TR < 1 or orig_TR > 3:\n",
    "                    #     print(f\"skipped due to orig_TR = {orig_TR}\")\n",
    "                    #     continue\n",
    "\n",
    "                    # zscore across the run\n",
    "                    out_shape = out.shape\n",
    "                    out = out.reshape(len(out),-1)\n",
    "                    scalar = StandardScaler(with_mean=True, with_std=True).fit(out)\n",
    "                    mean = scalar.mean_\n",
    "                    sd = scalar.scale_\n",
    "                    out = (out - mean) / sd\n",
    "                    mean = mean.reshape([out_shape[1],out_shape[2],out_shape[3]])\n",
    "                    sd = sd.reshape([out_shape[1],out_shape[2],out_shape[3]])\n",
    "                    meansd = np.array([mean,sd])\n",
    "                    out = out.reshape(out_shape)\n",
    "\n",
    "                    # create 16-bit png of mean and sd volumes\n",
    "                    meansd_images = reshape_to_2d(meansd)\n",
    "                    meansd_images = torch.Tensor(meansd_images)\n",
    "                    min_meansd, max_meansd = meansd_images.min(), meansd_images.max()\n",
    "                    minmax_meansd_images = (meansd_images - min_meansd) / (max_meansd - min_meansd) # first you need to rescale to 0 to 1\n",
    "                    rescaled_images = (minmax_meansd_images * 65535).to(torch.int16) # then multiply by constant prior to numpy uint16\n",
    "                    rescaled_images_numpy = rescaled_images.numpy().astype(np.uint16)\n",
    "                    meansd_PIL_image = Image.fromarray(rescaled_images_numpy, mode='I;16')\n",
    "\n",
    "                    # create samples of TRs_per_sample TRs\n",
    "                    for batch in range(0,len(out),TRs_per_sample):\n",
    "                        if len(out[batch:batch+TRs_per_sample])<TRs_per_sample:\n",
    "                            continue\n",
    "                        images = reshape_to_2d(out[batch:batch+TRs_per_sample])\n",
    "                        images = torch.Tensor(images)\n",
    "\n",
    "                        # convert tensor to something compatible with 16-bit png\n",
    "                        min_, max_ = images.min(), images.max()\n",
    "                        minmax_images = (images - min_) / (max_ - min_) # first you need to rescale to 0 to 1\n",
    "                        rescaled_images = (minmax_images * 65535).to(torch.int16) # then multiply by constant prior to numpy uint16\n",
    "                        rescaled_images_numpy = rescaled_images.numpy().astype(np.uint16)\n",
    "                        PIL_image = Image.fromarray(rescaled_images_numpy, mode='I;16')\n",
    "\n",
    "                        sink.write({\n",
    "                            \"__key__\": \"%06d\" % sample_idx,\n",
    "                            \"dataset.txt\": obj_key,\n",
    "                            \"header.npy\": np.array(header_to_dict(func_nii.header)),\n",
    "                            \"minmax.npy\": np.array([min_, max_, min_meansd, max_meansd]),\n",
    "                            \"meansd.png\": meansd_PIL_image,\n",
    "                            \"func.png\": PIL_image, # 27M for 8-bit png vs 48M for 16-bit png vs 144M for numpy\n",
    "                        })\n",
    "\n",
    "                        if current_dataset != obj['Key'].split('/')[0]:\n",
    "                            print(obj_key, \"| TR_count:\", TR_count, \"sample_idx:\", sample_idx)\n",
    "                            dataset_list.append(obj['Key'].split('/')[0])\n",
    "                            dataset_count += 1\n",
    "                            if is_interactive(): # dont want to plot unless you are in interactive notebook\n",
    "                                torchio_slice(out) # plot normalized slices\n",
    "                                torchio_slice((out * sd) + mean) # plot unnormalized slices\n",
    "\n",
    "                        if current_subject != obj['Key'].split('/')[1]:\n",
    "                            subj_count += 1\n",
    "                        current_dataset = obj['Key'].split('/')[0]\n",
    "                        current_subject = obj['Key'].split('/')[1]\n",
    "\n",
    "                        TR_count += TRs_per_sample\n",
    "                        sample_idx += 1\n",
    "\n",
    "                        if sample_idx >= max_samples_per_tar:\n",
    "                            print(\"HIT MAX SAMPLES PER TAR\")\n",
    "                            sink.close()\n",
    "                            sample_idx = 0\n",
    "\n",
    "                            # make metadata file and save progress to aws s3\n",
    "                            data = {\n",
    "                                \"TR_count\": TR_count,\n",
    "                                \"subj_count\": subj_count,\n",
    "                                \"tar_count\": tar_count,\n",
    "                                \"dataset_count\": dataset_count,\n",
    "                                \"datasets\": dataset_list,\n",
    "                                \"obj_key_list\": obj_key_list,\n",
    "                                \"failed_datasets\": failed_dataset_dict\n",
    "                            }\n",
    "                            with open(f\"{outpath}/tars/metadata.json\", \"w\") as file:\n",
    "                                json.dump(data, file)\n",
    "\n",
    "                            # send to aws s3\n",
    "                            command = f\"aws s3 sync {outpath}/tars s3://proj-fmri/fmri_foundation_datasets/conscioustahoe/{proj_name} --region us-west-2\"\n",
    "                            call(command,shell=True)\n",
    "\n",
    "                            # delete tars\n",
    "                            command = f\"rm {outpath}/tars/*.tar\"\n",
    "                            call(command,shell=True)\n",
    "\n",
    "                            tar_count += 1\n",
    "                            sink = wds.TarWriter(f\"{outpath}/tars/{tar_count:06d}.tar\")\n",
    "\n",
    "                    obj_key_list.append(obj['Key'])\n",
    "\n",
    "                    # delete the nifti now that youve saved it to numpy\n",
    "                    call(f\"rm {filename}\",shell=True)\n",
    "    except Exception as e:\n",
    "        failed_dataset = folder_name\n",
    "        trace_str = traceback.format_exc()\n",
    "        print(f\"Error occurred while processing dataset {failed_dataset}: {e}\")\n",
    "        failed_dataset_dict[failed_dataset] = trace_str\n",
    "        tar_count = metadata['tar_count']\n",
    "    finally:\n",
    "        print(\"TR_count\",TR_count)\n",
    "        print(\"subj_count\",subj_count)\n",
    "        print(\"dataset_count\",dataset_count)\n",
    "        print(\"failed_dataset_count\", len(failed_dataset_dict.keys()))\n",
    "        print(\"tar_count\", tar_count)            \n",
    "        try:\n",
    "            sink.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        data = {\n",
    "            \"TR_count\": TR_count,\n",
    "            \"subj_count\": subj_count,\n",
    "            \"tar_count\": tar_count,\n",
    "            \"dataset_count\": dataset_count,\n",
    "            \"datasets\": dataset_list,\n",
    "            \"obj_key_list\": obj_key_list,\n",
    "            \"failed_datasets\": failed_dataset_dict            \n",
    "        }\n",
    "\n",
    "        with open(f\"{outpath}/tars/metadata.json\", \"w\") as file:\n",
    "            json.dump(data, file)\n",
    "\n",
    "        # send to aws s3\n",
    "        command = f\"aws s3 sync {outpath}/tars s3://proj-fmri/fmri_foundation_datasets/conscioustahoe/{proj_name} --region us-west-2\"\n",
    "        call(command,shell=True)\n",
    "\n",
    "        # delete tars\n",
    "        command = f\"rm {outpath}/tars/*.tar\"\n",
    "        call(command,shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51af96f-d71b-4024-b52c-b1a12eca54d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to S3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            print(\"Starting job\")\n",
    "            dataset_creation_job()\n",
    "            print(\"Finished job\")\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f\"Job took: {elapsed_time} seconds\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        finally:\n",
    "            print(\"Resuming job in 5 seconds\")\n",
    "            time.sleep(5)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d19c259d-405c-4dc1-8661-dd245f4fcce8",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401771a1-9757-458b-999d-def515a6cd08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
